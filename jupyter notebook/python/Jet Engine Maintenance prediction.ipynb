from tensorflow import keras
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import seaborn as sns
from sklearn import preprocessing
from sklearn.metrics import mean_squared_error, mean_absolute_error
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Dropout, SimpleRNN, LSTM, GRU

# Set seed for reproducibility
np.random.seed(1234)
PYTHONHASHSEED = 0

# Training data where the last cycle is the failure point for the engines
train_df = pd.read_csv('t4train.txt', sep=" ", header=None)

# Test data where the failure point is not given for the engines
test_df = pd.read_csv('t4test.txt', sep=" ", header=None)

truth_df = pd.read_csv('t4truth.txt', sep=" ", header=None)

# We wil print some rows of the train data
train_df.head(100)

test_df.head(100)

truth_df.head(100)

# Let's drop the NAN columns 26 and 27 using the dropna() method.
train_df.dropna(axis=1, inplace=True)
test_df.dropna(axis=1, inplace=True)
print(len(train_df))
print(len(test_df))

cols_names = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',
                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',
                     's15', 's16', 's17', 's18', 's19', 's20' , 's21']

train_df.columns = cols_names
test_df.columns = cols_names

train_df.head(25)


truth_df = pd.read_csv('t4truth.txt', sep=" ", header=None)
truth_df.head()

truth_df.dropna(axis=1, inplace=True)
truth_df.head()



# Extract the maximum number of cycles for each engine id.
rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()

# Name the columns and merge them with the training data.
rul.columns = ['id', 'max']
train_df = train_df.merge(rul, on=['id'], how='left')

# Now subtract the current cycle from the max (maximum number of cycles) to calculate the remaining useful life.
train_df['RUL'] = train_df['max'] - train_df['cycle']

#The RUL is calculated for each id.
train_df[['id','cycle','max','RUL']].head()

train_df.drop('max', axis=1, inplace=True)
w2 = 30
train_df['failure_within_w1'] = np.where(train_df['RUL'] <= w2, 1, 0 )

train_df['cycle_norm'] = train_df['cycle']

# Using the difference function, we exclude these columns from the normalization process.
cols_normalize = train_df.columns.difference(['id','cycle','RUL','failure_within_w1'])

# MinMax normalization (from 0 to 1) of sensor data
min_max_scaler = preprocessing.MinMaxScaler()
norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]), 
                             columns=cols_normalize, 
                             index=train_df.index)

#Join the normalized and non-normalized data.
join_df = train_df[['id','cycle','RUL','failure_within_w1']].join(norm_train_df)
train_df = join_df.reindex(columns = train_df.columns)

train_df.head()

# 2. Scatter Plot of Sensor Readings vs. Cycles
plt.figure(figsize=(15, 10))
for i, col in enumerate(train_df.columns[5:]):
    plt.subplot(5, 5, i+1)
    sns.scatterplot(x='cycle', y=col, data=train_df)
    plt.title(col)
plt.tight_layout()
plt.show()


# 3. Time Series Plots for Specific Engines
engine_id = 1
plt.figure(figsize=(10, 6))
for col in train_df.columns[5:]:
    plt.plot(train_df[train_df['id'] == engine_id]['cycle'], train_df[train_df['id'] == engine_id][col], label=col)
plt.xlabel('Cycle')
plt.ylabel('Sensor Readings')
plt.title(f'Time Series for Engine {engine_id}')
plt.legend()
plt.show()

sensor_cols = ['s1', 's2', 's3', 's4', 's17', 's18', 's19', 's20', 's21']

# Create a figure and axes (2 graphs in a row)
fig, axes = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows, 5 columns

# Plot each sensor reading over time (cycle)
for i, col in enumerate(sensor_cols):
    row = i // 5  # Calculate row index
    col_idx = i % 5  # Calculate column index
    axes[row, col_idx].plot(train_df['cycle'], train_df[col], label=col, color=f"C{i}")
    axes[row, col_idx].set_xlabel('Cycle')
    axes[row, col_idx].set_ylabel(f'{col} (Normalized)')
    axes[row, col_idx].legend()

# Adjust spacing and display the plot
plt.tight_layout()
plt.show()

# 1. Load the data
data = pd.read_csv('t4train.csv')  # Replace 't4train.csv' with your actual file name

# 2. Define sensor columns
sensor_cols = ['s1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10', 
              's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20'] 

# 3. Create a MinMaxScaler object
scaler = MinMaxScaler()

# 4. Fit the scaler to the sensor data and transform
norm_data = pd.DataFrame(scaler.fit_transform(data[sensor_cols]),
                        columns=sensor_cols, 
                        index=data.index)

# 5. Select data for id=1 and normalize it
norm_data_id1 = norm_data[data['id'] == 1]

# 6. Plot the normalized data for id=1
norm_data_id1[sensor_cols].plot(figsize=(20, 8))
plt.show()

# 1. Load the data
data = pd.read_csv('t4train.csv')  # Replace 't4train.csv' with your actual file name

# 2. Define sensor columns
sensor_cols = ['s1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10', 
              's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20'] 

# 3. Create a MinMaxScaler object
scaler = MinMaxScaler()

# 4. Fit the scaler to the sensor data and transform
norm_data = pd.DataFrame(scaler.fit_transform(data[sensor_cols]),
                        columns=sensor_cols, 
                        index=data.index)

# 5. (Optional) Add other columns back to the normalized DataFrame
# Example: If your DataFrame has 'id' and 'cycle' columns as well:
# norm_data['id'] = data['id']
# norm_data['cycle'] = data['cycle']

# 6. Save the normalized data to a new CSV file (optional)
# norm_data.to_csv('t4train_normalized.csv', index=False)

# 7. Display the normalized DataFrame (optional)
print(norm_data) 

# 1. Load the data
data = pd.read_csv('t4train.csv')  # Replace 't4train.csv' with your actual file name

# 2. Define sensor columns
sensor_cols = ['s1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10', 
              's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20'] 

# 3. Create a MinMaxScaler object
scaler = MinMaxScaler()

# 4. Fit the scaler to the sensor data and transform
norm_data = pd.DataFrame(scaler.fit_transform(data[sensor_cols]),
                        columns=sensor_cols, 
                        index=data.index)

colors = ['#FF5733', '#3498DB', '#2ECC71', '#F012BE', '#FFC300', '#00FFFF', '#800080', '#008000', 
           '#FF6347', '#90EE90', '#00BFFF', '#FF1493', '#00CED1', '#708090', '#7CFC00', '#D2691E', '#008B8B', 
           '#A52A2A', '#808080', '#FFD700']  # Add more colors

# Adjust the figure size here
fig, axes = plt.subplots(10, 2, figsize=(15, 25))  # Increase width and height

for i, col in enumerate(sensor_cols):
    row = i // 2  
    col_idx = i % 2 

    axes[row, col_idx].plot(norm_data.index, norm_data[col], label=f'{col} (Normalized)', 
                            color=colors[i], alpha=0.7) 

    # Customize plot appearance
    axes[row, col_idx].set_xlabel('Time')
    axes[row, col_idx].set_ylabel(f'{col} Value')
    axes[row, col_idx].legend()
    axes[row, col_idx].set_title(f'Sensor {col}')

plt.tight_layout()
plt.show()

data = pd.read_csv('t4train.csv')
norm_data[data['id'] == 1]['s2'].plot(figsize=(10, 3),color='green')
plt.show()

data = pd.read_csv('t4train.csv')
train_df[train_df.id==5][sensor_cols[2]].plot(figsize=(10, 3), color='orange')
plt.show()

data = pd.read_csv('t4train.csv')
train_df[train_df.id==1][sensor_cols[6]].plot(figsize=(10, 3),color='red')
plt.show()

data = pd.read_csv('t4test.csv')
train_df[train_df.id==50][sensor_cols[6]].plot(figsize=(10, 3),color='blue')
plt.show()

def plot_model_accuracy(model_history, width=10, height=5):
  """Plots the training and validation accuracy of a Keras model."""
  fig_acc = plt.figure(figsize=(width, height))
  plt.plot(model_history.history['accuracy'])
  plt.plot(model_history.history['val_accuracy'])
  plt.title('Model Accuracy')
  plt.ylabel('Accuracy')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'Validation'], loc='upper left')
  plt.show()

# 1. Load the data
data = pd.read_csv('t4train.csv')  

# 2. Define features (sensor columns) and target (e.g., 'cycle')
sensor_cols = ['s1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10', 
              's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20']
target_col = 'cycle' 

# 3. Split the data into training and testing sets
X = data[sensor_cols]
y = data[target_col]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) 

# 4. Create a simple neural network model with Dropout for regularization
model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))
model.add(Dropout(0.2))  # Add dropout layer (20% dropout rate)
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))  # Add another dropout layer
model.add(Dense(1))  # Output layer with 1 neuron for regression

# 5. Compile the model
model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])

# 6. Set up early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# 7. Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=32, 
                    validation_data=(X_test, y_test), callbacks=[early_stopping])

# 8. Plot the model accuracy
plot_model_accuracy(history) 

# 1. Load the data
data = pd.read_csv('t4train.csv')  # Update the path to your file

# 2. Define features (sensor columns) and target (Remaining Useful Life, RUL)
sensor_cols = ['s1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10', 
              's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20']
# Calculate RUL as the maximum cycle minus the current cycle
data['RUL'] = data['cycle'].max() - data['cycle']

# 3. Split the data into training and testing sets
X = data[sensor_cols]
y = data['RUL']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. Feature Scaling (Standardization)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)  # Apply the same scaling to the test set

# 5. Define time_steps and reshape the data
time_steps = 10  # You can adjust this value based on your data
n_features = X_train.shape[1]

def create_3d_array(X, time_steps, n_features):
    X_3d = []
    for i in range(X.shape[0] - time_steps + 1):
        X_3d.append(X[i:i+time_steps])
    return np.array(X_3d)

X_train = create_3d_array(X_train, time_steps, n_features)
X_test = create_3d_array(X_test, time_steps, n_features)

# Adjust y_train and y_test to match the length of the reshaped X_train and X_test
y_train = y_train[time_steps-1:]
y_test = y_test[time_steps-1:]

# 6. Create an LSTM model
model = Sequential()
model.add(LSTM(128, return_sequences=True, input_shape=(time_steps, n_features)))
model.add(Dropout(0.3))
model.add(LSTM(64))
model.add(Dropout(0.3))
model.add(Dense(1))  # Output layer with 1 neuron for regression

# 7. Compile the model
model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])

# 8. Set up early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# 9. Train the model
history = model.fit(X_train, y_train, epochs=150, batch_size=32, 
                    validation_data=(X_test, y_test), callbacks=[early_stopping])

# Make predictions on the test set
y_pred = model.predict(X_test)

# Create a plot comparing predicted vs actual data
plt.figure(figsize=(10, 5))
plt.plot(y_test, label='Actual', color='blue')
plt.plot(y_pred, label='Predicted', color='red')
plt.title('Actual vs. Predicted Remaining Useful Life')
plt.ylabel('Remaining Useful Life (Cycles)')
plt.xlabel('Data Point')
plt.legend()
plt.show()

# Calculate and print mean squared error
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
